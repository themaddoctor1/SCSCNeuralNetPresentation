\documentclass[11pt]{beamer}
\usetheme{metropolis}           % Use metropolis theme

\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}

\graphicspath{ {/home/chrishittner/Desktop/} }

\title{Neural Networks}
\author{Christopher Hittner}
%\date{\today}
%\institute{Centre for Modern Beamer Themes}
\begin{document}
\maketitle
\section{Introduction}

\begin{frame}{(Kinda) Needed Concepts}
We will briefly utilize the following mathematical concepts:
\begin{itemize}
\item Addition
\item Multiplication
\item Matrices
\item Derivatives
\end{itemize}
\end{frame}

\begin{frame}{History}
\begin{itemize}
    \item Donald Hebb - The Organization of Behavior (1949)
    \begin{itemize}
        \item Defined a form of neural network where connections that are frequently used are strengthened.
    \end{itemize}
    \item Bernard Widrow and Marcian Hoff (Stanford Univ., 1959)
    \begin{itemize}
        \item Developed the theory behind ADALINE (ADAptive LINEardi Neuron) Networks.
        \item Neural networks that sum a set of inputs and returns a binary response based on its firing threshold.
    \end{itemize}
    \item Perceptrons: an introduction to computational geometry (1969)
    \begin{itemize}
        \item Published by Marvin Minsky and Seymour Papert.
        \item Suggested that Neural Networks could not handle the XOR problem.
        \item Contributed to the first AI winter.
    \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Recent History}
\begin{itemize}
    \item AlphaGo
    \begin{itemize}
        \item Google AI capable of beating world champions in Go.
        \item Uses Monte Carlo tree search with probabilities generated by a neural net.
    \end{itemize}
    \item Google DeepDream
    \begin{itemize}
        \item Uses Convolutional Nets to enhance parts of images.
        \item Similar methodology to the backpropagation algorithm
        \item \url{https://deepdreamgenerator.com/}
    \end{itemize}
    \item Deep Photo Style Transfer
    \begin{itemize}
        \item Convolutional Neural Net that modifies the style of images.
        \item \url{https://github.com/luanfujun/deep-photo-styletransfer}
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Sample Dreams}
\begin{figure}
\centering
\begin{subfigure}[b]{.45\linewidth}
\includegraphics[width=\linewidth]{images/Chris.jpg}
\caption{My unaltered self}\label{fig:chris}
\end{subfigure}
\begin{subfigure}[b]{.45\linewidth}
\includegraphics[width=\linewidth]{images/VanGoghFilter.jpg}
\caption{Me as a Van Gogh}\label{fig:vangogh}
\end{subfigure}
\begin{subfigure}[b]{.45\linewidth}
\includegraphics[width=\linewidth]{images/DaVinciFilter.jpg}
\caption{Me as a DaVinci}\label{fig:davinci}
\end{subfigure}
\begin{subfigure}[b]{.45\linewidth}
\includegraphics[width=\linewidth]{images/DogeFilter.jpg}
\caption{Such dream. Wow.}\label{fig:davinci}
\end{subfigure}
%\caption{Sample usage of DeepDream}
\label{fig:dreams}
\end{figure}
\end{frame}

\section{Neural Networks}

\begin{frame}{Definition}
\begin{itemize}
    \item A Neural Network is a computational model inspired by the human brain.
    \item Made of layers of neurons and synapses between them.
    \item Neuron - Unit that takes input and send output through synapses.
    \item Synapse - Connection between neurons.
\end{itemize}
\end{frame}

\begin{frame}{Mathematical Model}
Neural Networks consist of neurons with weights between them. \\
(ex: $n_i$ and $n_j$ with weight $w_{ji}$ between them). \\
Every neuron has input and output activations based on the weights and neuron outputs: \\
$s_j = \sum_{i = 0}^{N - 1} w_{ji}x_i$ \\
$y_j = f_j(s_j)$, where $f_j$ is the activation function of $n_j$, which is shared across its entire layer. \\
For neurons in the input layer, the $x_i$ values will instead be the inputs to the network. Elsewhere, they will be the outputs from the previous layer.
\end{frame}

\begin{frame}{Mathematical Model}
Linear Algebra can be used to represent the model in the form of matrices and vectors: \\
$\vec{s}_i = W_i \vec{x}_i$ \\
$\vec{y}_i = f_i(\vec{s}_i)$ \\
In this case, i represents the layer for which the values are being computed, and $\vec{x}_i$ can be either the network input $\vec{x}$ or the previous layer's output $\vec{y}_{i-1}$.
\end{frame}

\section{Neural Network Models}

\begin{frame}{Types of Neural Networks}
There are numerous variations of the neural network model:
\begin{itemize}
    \item Feedforward Neural Net
    \begin{itemize}
        \item Perceptron
        \item ADALINE
        \item Convolutional Neural Net
    \end{itemize}
    \item Recurrent Neural Net
    \begin{itemize}
        \item Hopfield Networks
        \item Jordan/Elman Networks
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Feedforward Networks}
Activations feed forward sequentially through the network.
\begin{itemize}
    \item ADALINE (ADAptive LINear Element)
    \begin{itemize}
        \item Developed by Widrow and Hoff in 1959.
        \item Single layer network that uses the unit step function as its activation function.
    \end{itemize}
    \item Perceptron
    \begin{itemize}
        \item Developed by Frank Rosenblatt in 1957.
        \item Generally uses unit step as its activation function.
        \item Can be expanded to multiple layers.
    \end{itemize}
    \item Both use gradient descent to derive error in training.
\end{itemize}
\end{frame}

\begin{frame}{Recurrent Neural Networks}
Network that uses repeating layers to compute results.
\begin{itemize}
    \item Hopfield Network
    \begin{itemize}
        \item Output activations update continuously until stable value is reached.
        \item At time i, $\vec{y}_{i+1} = W \vec{y}_i$
        \item Network execution will converge on a value.
    \end{itemize}
    \item Elman Network
    \begin{itemize}
        \item Takes a set of time-indexed vectors $\vec{x}_i$.
        \item At time i, $\vec{s}_{i+1} = f(W\vec{x}_i + R\vec{s}_i)$
        \item $\vec{y}_i = f(W\vec{s}_{i+1})$
        \item Jordan Network is similar, but uses $\vec{y}_i$ instead of $\vec{s}_i$ to compute $\vec{s}_{i+1}$.
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Convolutional Neural Networks}
\begin{itemize}
    \item Networks based on the function of the visual cortex.
    \item Use filters spanning input to interpret and reduce data to an answer.
    \item Applies ReLU function to filter outputs.
\end{itemize}
\end{frame}

\section{Training Algorithms}

\begin{frame}{Training Algorithms}
Training algorithms are procedures used to teach neural networks to correctly respond to inputs. A few examples of training algorithms include:
\begin{itemize}
    \item Perceptron Rule
    \begin{itemize} \item Learning rule for single layer perceptrons. \end{itemize}
    \item Hebbian Rule
    \begin{itemize} \item Rule based on conditioning that does not require supervision. \end{itemize}
    \item Backpropagation
    \begin{itemize} \item General algorithm for multilayer networks. \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Perceptron Rule}
Training rule used to train perceptrons to correctly classify inputs. \\
Given an input vector $\vec{x}$ and a target vector $\vec{t}$:
\begin{enumerate}
    \item Compute the output $\vec{y}$ of the network.
    \item Determine the error $\vec{e} = \vec{t} - \vec{y}$.
    \item Update the weight matrix $W' = W + \vec{e} * \vec{x}^T$.
\end{enumerate}
This process can be repeated for as many training pairs as necessary, and for as many rounds as needed. \\
Limitation: Can only be used on linearly separable data.
\end{frame}

\begin{frame}{Backpropagation}
\begin{itemize}
    \item Training algorithm for multilayer networks that takes advantage of the chain rule to propagate error through the network. \\
    \item We say that the error $E = 0.5(t - y)^2$ so that $\vec{e} = \frac{\partial \vec{E}}{\partial \vec{y}} = \vec{y} - \vec{t}$.
    \item We seek to determine $\frac{\partial \vec{E}}{\partial W}$ for each weight matrix W. \\
    \item On each layer, we compute a sum vector $\vec{s}$ and an output vector $\vec{y} = f(\vec{s})$ in order to feed forward. \\
    \item Error is propagated backwards by determining $\frac{\partial \vec{y}_i}{\partial \vec{s}_i}$ for each layer and $\frac{\partial \vec{s}_i}{\partial \vec{y}_{i-1}}$ between each layer. \\
    \item To determine change to each weight matrix, we must compute $\frac{\partial \vec{s}_i}{\partial W_i}$ for each matrix.
\end{itemize}
\end{frame}

\begin{frame}{Backpropagation}
The needed derivatives are as follows: \\
\[
\frac{\partial \vec{y}_i}{\partial \vec{s}_i} = \frac{\partial f_i}{\partial \vec{s}_i} = 
\begin{bmatrix}
    f'(s_{i,0}) & 0            & \dots   & 0      \\
    0           & f'(s_{i,1})  & \dots   & 0      \\
    \vdots      & \vdots       & \ddots  & \vdots \\
    0           & 0            & \dots   & f'(s_{i,n-1}) \\
\end{bmatrix}
\]

$\frac{\partial \vec{s}_i}{\partial \vec{y}_{i-1}} = \frac{\partial}{\partial \vec{y}_{i-1}} W\vec{y}_{i-1} = W_i^T$ \\
$\frac{\partial \vec{s}_i}{\partial W_i} = x_i^T$ \\
Using these equations, we can use the chain rule to propagate error through a network.
\end{frame}

\begin{frame}{Backpropagation}
\begin{itemize}
    \item Vanishing gradient problem
    \begin{itemize}
        \item Propagated error approaches zero as it passes through multiple layers.
    \end{itemize}
    \item Multi-level hierarchies
    \begin{itemize}
    	\item Train layers separately to solve parts of a problem.
    \end{itemize}
    \item Long Short Term Memory
    \begin{itemize} 
        \item Networks that can remember values for short or long periods of time.
    \end{itemize}
    \item Residual Networks
    \begin{itemize} 
        \item Passing the network inputs into deeper layers.
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Hebbian Rule}
\begin{itemize}
\item Associative learning rule that can be supervised or unsupervised. \\
\item In the supervised version, the weight matrix is continuously updated like so: $W' = W + \vec{t} * \vec{x}^T$ \\
\item In the unsupervised version, $W' = W + \vec{y} * \vec{x}^T$. \\
\item Limitation: Has difficulty with non-orthogonal inputs.
\end{itemize}
\end{frame}

\begin{frame}{Kohonen Rule (Self-organizing Map)}
\begin{itemize}
    \item Unsupervised learning model based on Hebbian rule. \\
    \item Generates a low-dimensional representation of data. \\
    \item First layer builds a linear sum from the input vector. \\
    \item Second layer is a recurrent layer that applies ReLU function. \\
    \begin{itemize}
        \item Steady-state result of second layer is the end result. \\
        \item We call this a competitive layer.
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Kohonen Rule (cont.)}
In competitive layer, we have the following properties: \\
\begin{align*}
W_{ij} &= 1\text{ if }i == j\text{ else }-\epsilon \\
\text{where }0 &< \epsilon < \frac{1}{S-1} \\
S &= \text{layer size} \\
f(\vec{x}) &= relu(\vec{x}) = x\text{ if }x > 0\text{ else }0 \\
\end{align*}
The result of this is that any input will decrease until only one
dimension of input has a non-zero value, resulting in a steady-state
solution.
\end{frame}

\begin{frame}{Kohonen Rule (cont.)}
In training the network, we seek to find vectors that categorize the data. \\
We compute output $\vec{y}$ such that only one dimension of $\vec{y}$ is nonzero. \\
Then, $\vec{W}_{row i}' = \vec{W}_{row i} + \alpha (\vec{y} - \vec{w}_{row i})$, where $\alpha$ is the learning rate. \\
This causes row $i$ to become closer to the input vector $\vec{x}$.
\end{frame}


\begin{frame}{Additional Material}
\begin{itemize}
    \item[] \textit{Intelligent Machinery, A Heretical Theory}
        \begin{itemize} \item[] Alan Turing, 1948 \end{itemize}
    \item[] \textit{The Organization of Behavior}
        \begin{itemize} \item[] Donald Hebb, 1949 \end{itemize}
    \item[] \textit{An Adaptive “ADALINE” Neuron Using Chemical “Memsistors”}
        \begin{itemize} \item[] Bernard Widrow, 1960 \end{itemize}
    \item[] \textit{Mastering the Game of Go with Deep Neural Networks and Tree Search}
        \begin{itemize} \item[] Google DeepMind, 2016 \end{itemize}
    \item[] \textit{Neural Net Design}
        \begin{itemize} \item[] Martin Hagan, Howard Demuth, Mark Hudson Beale, and Orlando De Jesus \end{itemize}
\end{itemize}
\end{frame}

\end{document}
